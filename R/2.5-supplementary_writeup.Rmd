---
title: "Automation Failures in ATC: Standard Results"
author: "ljgs"
date: "20/11/2019"
output:
  word_document: default
  html_document: default
  pdf_document: default
---
by Luke Strickland

```{r load_packages_and_data, echo= FALSE , results = "hide", message=FALSE, warning=FALSE}

library(dplyr)
library(ggplot2)
library(lme4)
library(car)
library(lsmeans)
library(pander)
options(digits=2)
source("R/0-analysis_functions.R")

load("img/cleandats.RData")

cleandats <- cleandats %>% mutate(C = toupper(S)==R)

colnames(cleandats)[colnames(cleandats)=="sess"] <- "Session"
colnames(cleandats)[colnames(cleandats)=="cond"] <- "Condition"
colnames(cleandats)[colnames(cleandats)=="S"] <- "Stimulus"
colnames(cleandats)[colnames(cleandats)=="failtrial"] <- "Automation"

cleandats$Session <- factor(cleandats$Session, levels=c("1", "2"),
                      labels=c("One", "Two"))

cleandats$Condition <- factor(cleandats$Condition , levels=c("AUTO", "MANUAL"),
                      labels=c("Automation", "Manual"))

cleandats$Automation <- factor(cleandats$Automation, levels=c("nonf", "fail"),
                      labels=c("Automation Success", "Automation Failure"))

cleandats$Stimulus <- factor(cleandats$Stimulus, levels=c("c", "n"),
                      labels=c("Conflict", "Non-conflict"))

theme_set(theme_simple())

accs <-
  cleandats %>% group_by(s, Stimulus, Condition, Automation, Session) %>% 
  filter(!is.na(R)) %>% summarise(acc = mean(C)) %>%
  arrange(s) %>% arrange(Automation)

RTs <- cleandats %>% group_by(s, Stimulus, Condition, Automation, Session) %>% 
  filter(C) %>% 
  summarise(RT=mean(RT))%>% arrange(Automation)

```

Table S1

*Wald Chi-Square significance tests for conflict detection accuracy. A generalized linear
mixed-effects model was fitted to every trial, with a binomial probit link 
function. Random intercepts were included for each
participant. In addition to examining experimental condition (automated vs manual),
'Automation' (a factor denoting whether the automation succeeded or failed)
and stimulus type (conflict/non-conflict), 
we included a 'session' factor to account for
effects of task repetition.*

```{r accuracy_model, echo= FALSE, message=TRUE, warning=TRUE, results="asis"}
# 
# acc_glmer_top <-
#   glmer(C ~ Stimulus * Condition * Session * Automation + (1 |s),
#         data = cleandats,
#         family = binomial(link = "probit"))
# 
# ss <- getME(acc_glmer_top,c("theta","fixef"))
# acc_glmer_top2 <- update(acc_glmer_top,
#                              start = ss,
#                              control = glmerControl(optCtrl = list(maxfun = 2e4)))
# 
# 
# ss2 <- getME(acc_glmer_top2,c("theta","fixef"))
# acc_glmer_top3 <- update(acc_glmer_top2,
#                              start = ss2,
#                              control = glmerControl(optCtrl = list(maxfun = 2e5)))

# save(acc_glmer_top3, file = "img/acc_model.RData")

load("img/acc_model.RData")

pandoc.table(
  make_model_table(Anova(acc_glmer_top3,type="II")))

```

Table S2

*Follow up significance tests of the difference in conflict detection accuracy
across conditions, conditional on whether the automation succeeded or failed. 
The z.ratio is the mean of the effect divided by its standard error.
Tukey adjustments were applied to the included p-values.*

```{r ldt_accuracy_trial_range, echo= FALSE, message=FALSE, warning=FALSE, results="asis",fig.height = 3, fig.width = 4}
make_contrast_table(summary(
  contrast(lsmeans(acc_glmer_top3, ~Condition|Automation), 
           method="pairwise")) %>% select(-df))

```


Table S3

*Wald Chi-Square significance tests for ongoing task RT. A linear
mixed-effects model was fitted to mean correct PM RTs. Random intercepts were included for each
participant. In addition to examining experimental condition (automated vs manual),
'Automation' (a factor denoting whether the automation succeeded or failed)
and stimulus type (conflict/non-conflict), 
we included a 'session' factor to account for
effects of task repetition.*

```{r RT_model, echo= FALSE, message=TRUE, warning=TRUE, results="asis"}

RT_model <- lmer(RT ~ Stimulus * Condition * Session * Automation + (1 |s),
     data = RTs)

pandoc.table(
  make_model_table(Anova(RT_model, type="II")))

```

Table S4

*Follow up significance tests of the differences in RT across
experimental conditions, conditional on whether the automation
succeeded or failed. Tukey adjustments were applied to the included p-values.*

```{r RT_contrasts_1, echo= FALSE, message=FALSE, warning=FALSE, results="asis"}

make_contrast_table(summary(
  contrast(lsmeans(RT_model, ~Condition|Automation), 
           method="pairwise")))

```

Table S5

*Follow up significance tests of the differences in RT across
experimental conditions, conditional on whether it was session 1 
or session 2 of the experiment. Tukey adjustments were applied to the included p-values.*


```{r RT_contrasts_2, echo= FALSE, message=FALSE, warning=FALSE, results="asis"}
make_contrast_table(summary(
  contrast(lsmeans(RT_model, ~Condition|Session),
           method="pairwise")))

```


Table S5

*Follow up significance tests of the differences in RT across
experimental conditions, conditional on whether it was session 1 
or session 2 of the experiment, and
whether the automation provided the correct or incorrect
recommendation. Tukey adjustments were applied to the included p-values.*

```{r RT_contrasts_3, echo= FALSE, message=FALSE, warning=FALSE, results="asis"}
make_contrast_table(summary(
  contrast(lsmeans(RT_model, ~Condition|Session * Automation),
           method="pairwise")))

```
